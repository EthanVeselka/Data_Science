{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for each Topic (a)\n",
      "    Topic 1 Topic 2  Topic 3   Topic 4 Topic 5  Topic 6  Topic 7  Topic 8  \\\n",
      "1      film    movi     film      film    movi     movi     book     plai   \n",
      "2      plai   watch     love       war     get     film     film     movi   \n",
      "3   perform    like    stori     peopl    like  charact     just     good   \n",
      "4      cast    just     life     world    just    stori    stori    great   \n",
      "5      role    time    young      time     bad     like     read    music   \n",
      "6      star   funni  charact      make   scene   realli     like     role   \n",
      "7   charact   think   famili       man    film     just   realli      get   \n",
      "8      john    good   beauti      take    kill     time   horror  perform   \n",
      "9      best   peopl    scene      even    look      end  version     well   \n",
      "10     well  realli     time  american  horror    watch     seem     best   \n",
      "\n",
      "   Topic 9 Topic 10  \n",
      "1     show     film  \n",
      "2      get     movi  \n",
      "3   episod      bad  \n",
      "4     like     like  \n",
      "5     seri     just  \n",
      "6     just     even  \n",
      "7     time     good  \n",
      "8     back      act  \n",
      "9    first    watch  \n",
      "10    anim     make  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# PART A #\n",
    "# This code takes 1-2 mins to run on my computer (which is fast) because of the pivot and lda training\n",
    "\n",
    "# Load data\n",
    "counts_columns = ['row_index', 'term_index', 'count']\n",
    "counts = pd.read_csv('counts.csv', header=None, names=counts_columns)\n",
    "\n",
    "with open('vocabulary.csv', 'r') as file:\n",
    "    words_row = file.readline().strip().split(',')\n",
    "\n",
    "# create word map from vocab\n",
    "vocabulary = pd.DataFrame(words_row, columns=['words'], index=range(1, len(words_row) + 1))\n",
    "\n",
    "# pivot to get a document-term matrix\n",
    "dtm = counts.set_index('row_index').pivot(columns='term_index', values='count').fillna(0)\n",
    "\n",
    "# convert to sparse matrix\n",
    "dtm_sparse = csr_matrix(dtm.values)\n",
    "\n",
    "# Fit LDA\n",
    "num_topics = 10  # Set the number of topics\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(dtm_sparse)\n",
    "\n",
    "# Function to create matrix of top words for topics\n",
    "def get_top_words_matrix(model, feature_names, n_top_words):\n",
    "    top_words_matrix = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        top_words_matrix.append(top_words)\n",
    "    return pd.DataFrame(top_words_matrix).transpose()\n",
    "\n",
    "n_top_words = 10\n",
    "feature_names = dtm.columns\n",
    "top_words_matrix = get_top_words_matrix(lda, feature_names, n_top_words)\n",
    "\n",
    "top_words_matrix.columns = [f\"Topic {i + 1}\" for i in range(num_topics)]\n",
    "top_words_matrix.index = [f\"{i + 1}\" for i in range(n_top_words)]\n",
    "\n",
    "# map words to matrix and display\n",
    "print(\"Top 10 words for each Topic (a)\")\n",
    "mapped = top_words_matrix.applymap(lambda idx: vocabulary.loc[idx, 'words'])\n",
    "print(mapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for each Topic after filtering (b)\n",
      "   Topic 1     Topic 2  Topic 3  Topic 4  Topic 5    Topic 6  Topic 7  \\\n",
      "1     danc        book     seri    worst     john      enjoi   murder   \n",
      "2     song        read   episod     wast  michael        saw   killer   \n",
      "3    black     version   season    minut     jame        kid     hous   \n",
      "4    white      script     anim   stupid     town  recommend   beauti   \n",
      "5     sing       novel     last    zombi  western     wonder    becom   \n",
      "6   number       adapt    minut    monei      big     beauti    woman   \n",
      "7     rock        line      run  terribl      he'        fun      own   \n",
      "8     band     product  cartoon     bore   robert        fan  mysteri   \n",
      "9   school     dialogu   second  horribl      joe      alwai   viewer   \n",
      "10    high  disappoint    final    laugh   action     famili      art   \n",
      "\n",
      "    Topic 8      Topic 9   Topic 10  \n",
      "1    famili          war     effect  \n",
      "2       he'     american     action  \n",
      "3       kid        human    special  \n",
      "4      want        polit        fan  \n",
      "5    father          men  entertain  \n",
      "6    mother  documentari        bit  \n",
      "7      tell      histori     expect  \n",
      "8      need        women       idea  \n",
      "9   problem       differ        far  \n",
      "10    that'       person    monster  \n"
     ]
    }
   ],
   "source": [
    "# PART B #\n",
    "# This code takes ~1 min to run for the lda training\n",
    "\n",
    "top_words = dtm.sum(axis=0).nlargest(100).index.tolist()\n",
    "dtm_filtered = dtm.drop(columns=top_words)\n",
    "dtm_filtered_sparse = csr_matrix(dtm_filtered.values)\n",
    "\n",
    "num_topics = 10\n",
    "lda_filtered = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_filtered.fit(dtm_filtered_sparse)\n",
    "\n",
    "feature_names_filtered = dtm_filtered.columns\n",
    "n_top_words = 10\n",
    "top_words_matrix_filtered = get_top_words_matrix(lda_filtered, feature_names_filtered, n_top_words)\n",
    "\n",
    "top_words_matrix_filtered.columns = [f\"Topic {i + 1}\" for i in range(num_topics)]\n",
    "top_words_matrix_filtered.index = [f\"{i + 1}\" for i in range(n_top_words)]\n",
    "\n",
    "# map words to matrix and display\n",
    "print(\"Top 10 words for each Topic after filtering (b)\")\n",
    "mapped_filt = top_words_matrix_filtered.applymap(lambda idx: vocabulary.loc[idx, 'words'])\n",
    "print(mapped_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: array([0.552]), 2: array([0.554]), 3: array([0.528]), 4: array([0.47]), 5: array([0.528]), 6: array([0.508]), 7: array([0.532]), 8: array([0.508]), 9: array([0.46]), 10: array([0.492])}\n"
     ]
    }
   ],
   "source": [
    "# PART C #\n",
    "\n",
    "with open('sentiment.csv', 'r') as file:\n",
    "    sent_row = file.readline().strip().split(',')\n",
    "\n",
    "# create map from sentiment\n",
    "sentiment = pd.DataFrame(sent_row, columns=['Sentiment'], index=range(1, len(sent_row) + 1))\n",
    "sentiment = sentiment.to_numpy().astype(int)\n",
    "\n",
    "topic_distribution = lda_filtered.transform(dtm_filtered_sparse)\n",
    "dominant_topics = topic_distribution.argmax(axis=1)\n",
    "\n",
    "reviews_topics_df = pd.DataFrame({'ReviewIndex': range(1, len(dominant_topics) + 1), 'DominantTopic': dominant_topics + 1})\n",
    "grouped_reviews = reviews_topics_df.groupby('DominantTopic')['ReviewIndex'].apply(list)\n",
    "\n",
    "top_500_reviews_per_topic = {}\n",
    "for topic, reviews in grouped_reviews.items():\n",
    "    top_500_reviews_per_topic[topic] = reviews[:500]\n",
    "\n",
    "\n",
    "average_sentiments = {}\n",
    "for topic, indices in top_500_reviews_per_topic.items():\n",
    "    topic_sentiments = [sentiment[idx] for idx in indices]\n",
    "    average_sentiments[topic] = sum(topic_sentiments) / len(topic_sentiments)\n",
    "\n",
    "print(average_sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Words  Absolute_Difference\n",
      "1814          theatre - the         2.365198e-01\n",
      "1812             the - that         1.764906e-01\n",
      "89             and - amount         1.281453e-01\n",
      "93              angry - and         1.118331e-01\n",
      "250              brown - br         1.110023e-01\n",
      "...                     ...                  ...\n",
      "487    difference - details         6.397665e-06\n",
      "127         army - artistic         5.714663e-06\n",
      "117   appearance - appeared         4.896855e-06\n",
      "1443         putting - puts         3.809540e-06\n",
      "454       decides - dancing         8.775057e-07\n",
      "\n",
      "[2073 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "reviews = pd.read_csv('reviews.tsv', sep='\\t')\n",
    "\n",
    "X_positive = reviews[reviews['sentiment'] == 1]['review']\n",
    "X_negative = reviews[reviews['sentiment'] == 0]['review']\n",
    "\n",
    "vectorizer_positive = TfidfVectorizer(max_features=2073) #ensures we only model the 2073 most common words\n",
    "vectorizer_negative = TfidfVectorizer(max_features=2073)\n",
    "\n",
    "tfidf_matrix_positive = vectorizer_positive.fit_transform(X_positive)\n",
    "tfidf_matrix_negative = vectorizer_negative.fit_transform(X_negative)\n",
    "\n",
    "average_tfidf_positive = np.asarray(tfidf_matrix_positive.mean(axis=0)).flatten()\n",
    "average_tfidf_negative = np.asarray(tfidf_matrix_negative.mean(axis=0)).flatten()\n",
    "difference = np.abs(average_tfidf_positive - average_tfidf_negative)\n",
    "\n",
    "feature_names_positive = vectorizer_positive.get_feature_names_out()\n",
    "feature_names_negative = vectorizer_negative.get_feature_names_out()\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Words': feature_names_positive + \" - \" + feature_names_negative,\n",
    "    'Absolute_Difference': difference\n",
    "})\n",
    "\n",
    "results_sorted = results.sort_values(by='Absolute_Difference', ascending=False)\n",
    "\n",
    "print(results_sorted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
